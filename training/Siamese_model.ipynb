{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8004d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Reshape,  LSTM, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f888dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_folder):\n",
    "    X, y = [], []\n",
    "    class_labels = os.listdir(dataset_folder)\n",
    "    \n",
    "    for label, class_folder in enumerate(class_labels):\n",
    "        class_path = os.path.join(dataset_folder, class_folder)\n",
    "        mfcc_files = os.listdir(class_path)\n",
    "        \n",
    "        for mfcc_file in mfcc_files:\n",
    "            file_path = os.path.join(class_path, mfcc_file)\n",
    "            mfcc_data = np.load(file_path)\n",
    "            X.append(mfcc_data)\n",
    "            y.append(label)\n",
    "    \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d33643",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = \"E:\\\\UGRP\\\\npy\\\\mfcc\"\n",
    "#dataset_folder = \"E:\\\\UGRP\\\\npy\\\\sg\"\n",
    "#dataset_folder = \"E:\\\\UGRP\\\\npy\\\\cg\"\n",
    "\n",
    "X, y = load_dataset(dataset_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aefa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_siamese_network(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    reshaped_input = Reshape((2498, 13))(input_layer)\n",
    "    \n",
    "    x = LSTM(128, return_sequences=True)(input_layer)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    embedding = Dense(64, activation='sigmoid')(x)\n",
    "\n",
    "    return Model(input_layer, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cc69a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha=0.2):\n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    pos_dist = K.sum(K.square(anchor - positive), axis=-1)\n",
    "    neg_dist = K.sum(K.square(anchor - negative), axis=-1)\n",
    "    basic_loss = pos_dist - neg_dist + alpha\n",
    "    loss = K.maximum(basic_loss, 0.0)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c63aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (13, 2498)\n",
    "siamese_network = create_siamese_network(input_shape)\n",
    "siamese_network.compile(loss=triplet_loss, optimizer=Adam(0.0001))\n",
    "siamese_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a635cadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_triplets(X, y, num_triplets):\n",
    "    triplets = []\n",
    "    classes = np.unique(y)\n",
    "    \n",
    "    for _ in range(num_triplets):\n",
    "        anchor_class = random.choice(classes)\n",
    "        negative_class = random.choice([cls for cls in classes if cls != anchor_class])\n",
    "\n",
    "        anchor_idx = random.choice(np.where(y == anchor_class)[0])\n",
    "        positive_idx = random.choice(np.where(y == anchor_class)[0])\n",
    "        negative_idx = random.choice(np.where(y == negative_class)[0])\n",
    "\n",
    "        anchor = X[anchor_idx]\n",
    "        positive = X[positive_idx]\n",
    "        negative = X[negative_idx]\n",
    "\n",
    "        if anchor.ndim < 3:\n",
    "            anchor = np.expand_dims(anchor, axis=0)\n",
    "        if positive.ndim < 3:\n",
    "            positive = np.expand_dims(positive, axis=0)\n",
    "        if negative.ndim < 3:\n",
    "            negative = np.expand_dims(negative, axis=0)\n",
    "\n",
    "        triplets.append([anchor, positive, negative])\n",
    "\n",
    "    return np.array(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733c72ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "num_triplets = 10000000  \n",
    "triplets = generate_triplets(X, y, num_triplets)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for i in range(0, len(triplets), batch_size):\n",
    "        batch_triplets = triplets[i:i + batch_size]\n",
    "        anchor, positive, negative = zip(*batch_triplets)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            anchor_embeddings = siamese_network(anchor)\n",
    "            positive_embeddings = siamese_network(positive)\n",
    "            negative_embeddings = siamese_network(negative)\n",
    "\n",
    "            loss = triplet_loss(None, [anchor_embeddings, positive_embeddings, negative_embeddings])\n",
    "\n",
    "        gradients = tape.gradient(loss, siamese_network.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, siamese_network.trainable_variables))\n",
    "        losses.append(loss.numpy())\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58de63ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vec1, vec2):\n",
    "    return np.sqrt(np.sum((vec1 - vec2) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a50da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_a = np.random.normal(0, 1, (13, 2498))\n",
    "vector_b = -vector_a\n",
    "\n",
    "vector_a = vector_a.reshape(1, 13, -1)\n",
    "vector_b = vector_b.reshape(1, 13, -1)\n",
    "\n",
    "embedding1 = siamese_network.predict([vector_a, vector_b])\n",
    "embedding2 = siamese_network.predict([vector_a, vector_a])\n",
    "\n",
    "min_score = euclidean_distance(embedding1, embedding1)\n",
    "max_score = euclidean_distance(embedding1, embedding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d906e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_value(x, min_val=max_score, max_val=min_score):\n",
    "    return (x - min_val) / (max_val - min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edccda66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(vects):\n",
    "    embedding1 = siamese_network.predict(vects[0])\n",
    "    embedding2 = siamese_network.predict(vects[1])\n",
    "    distance = euclidean_distance(embedding1, embedding2)\n",
    "    return normalize_value(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd06629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_vector1 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\1.1 표절_29s_mfcc.npy')\n",
    "mfcc_vector2 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\1.1 표절 key 3_29s_mfcc.npy')\n",
    "mfcc_vector3 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\1.1 표절 key -3_29s_mfcc.npy')\n",
    "mfcc_vector4 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\1.1 표절 noise 1_29s_mfcc.npy')\n",
    "mfcc_vector5 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\1.1 표절 noise 2_29s_mfcc.npy')\n",
    "mfcc_vector6 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\1.2 표절_29s_mfcc.npy')\n",
    "\n",
    "mfcc_vector7 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\2.1 표절_29s_mfcc.npy')\n",
    "mfcc_vector8 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\2.1 표절 key -3_29s_mfcc.npy')\n",
    "mfcc_vector9 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\2.1 표절_29s_mfcc.npy')\n",
    "mfcc_vector10 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\2.1 표절 key -3_29s_mfcc.npy')\n",
    "mfcc_vector11 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\2.1 표절_29s_mfcc.npy')\n",
    "mfcc_vector12 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\2.2 표절_29s_mfcc.npy')\n",
    "\n",
    "mfcc_vector13 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\3.1 커버_29s_mfcc.npy')\n",
    "mfcc_vector14 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\3.1 커버 key 3_29s_mfcc.npy')\n",
    "mfcc_vector15 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\3.1 커버 key -3_29s_mfcc.npy')\n",
    "mfcc_vector16 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\3.1 커버 noise 1_29s_mfcc.npy')\n",
    "mfcc_vector17 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\3.1 커버 noise 2_29s_mfcc.npy')\n",
    "mfcc_vector18 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\3.2 커버_29s_mfcc.npy')\n",
    "\n",
    "mfcc_vector19 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\4.1 커버_29s_mfcc.npy')\n",
    "mfcc_vector20 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\4.1 커버 key 3_29s_mfcc.npy')\n",
    "mfcc_vector21 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\4.1 커버 key -3_29s_mfcc.npy')\n",
    "mfcc_vector22 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\4.1 커버 noise 1_29s_mfcc.npy')\n",
    "mfcc_vector23 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\4.1 커버 noise 2_29s_mfcc.npy')\n",
    "mfcc_vector24 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\4.2 커버_29s_mfcc.npy')\n",
    "\n",
    "mfcc_vector25 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\5.1 샘플링_29s_mfcc.npy')\n",
    "mfcc_vector26 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\5.1 샘플링 key 3_29s_mfcc.npy')\n",
    "mfcc_vector27 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\5.1 샘플링 key -3_29s_mfcc.npy')\n",
    "mfcc_vector28 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\5.1 샘플링 noise 1_29s_mfcc.npy')\n",
    "mfcc_vector29 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\5.1 샘플링 noise 2_29s_mfcc.npy')\n",
    "mfcc_vector30 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\5.2 샘플링_29s_mfcc.npy')\n",
    "\n",
    "mfcc_vector31 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\6.1 샘플링_29s_mfcc.npy')\n",
    "mfcc_vector32 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\6.1 샘플링 key 3_29s_mfcc.npy')\n",
    "mfcc_vector33 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\6.1 샘플링 key -3_29s_mfcc.npy')\n",
    "mfcc_vector34 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\6.1 샘플링 noise 1_29s_mfcc.npy')\n",
    "mfcc_vector35 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\6.1 샘플링 noise 2_29s_mfcc.npy')\n",
    "mfcc_vector36 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\6.2 샘플링_29s_mfcc.npy')\n",
    "\n",
    "mfcc_vector37 = np.load('C:\\\\Users\\\\haim1\\\\new_folder\\\\test_mfcc\\\\7.1 다른곡_29s_mfcc.npy')\n",
    "\n",
    "\n",
    "for i in range(1, 38):\n",
    "    globals()[f'mfcc_vector{i}'] =  globals()[f'mfcc_vector{i}'].reshape(1, 13, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526448a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Result of MFCC\\n\")\n",
    "\n",
    "print('[1.1 vs 1.1 key 3]\\유사도:', result([mfcc_vector1, mfcc_vector2]))\n",
    "print('[1.1 vs 1.1 key -3]\\n유사도:', result([mfcc_vector1, mfcc_vector3]))\n",
    "print('[1.1 vs 1.1 noise 1]\\n유사도:', result([mfcc_vector1, mfcc_vector4]))\n",
    "print('[1.1 vs 1.1 noise 2]\\n유사도:', result([mfcc_vector1, mfcc_vector5]))\n",
    "print('[1.1 vs 다른 노래]\\n유사도:', result([mfcc_vector1, mfcc_vector37]))\n",
    "print('[1.1 vs 1.2]\\n유사도:', result([mfcc_vector1, mfcc_vector6]))\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "print('[2.1 vs 2.1 key 3]\\n유사도:', result([mfcc_vector7, mfcc_vector8]))\n",
    "print('[2.1 vs 2.1 key -3]\\n유사도:', result([mfcc_vector7, mfcc_vector9]))\n",
    "print('[2.1 vs 2.1 noise 1]\\n유사도:', result([mfcc_vector7, mfcc_vector10]))\n",
    "print('[2.1 vs 2.1 noise 2]\\n유사도:', result([mfcc_vector7, mfcc_vector11]))\n",
    "print('[2.1 vs 다른 노래]\\n유사도:', result([mfcc_vector7, mfcc_vector37]))\n",
    "print('[2.1 vs 2.2]\\n유사도:', result([mfcc_vector7, mfcc_vector12]))\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "print('[3.1 vs 3.1 key 3]\\n유사도:', result([mfcc_vector13, mfcc_vector14]))\n",
    "print('[3.1 vs 3.1 key -3]\\n유사도:',result([mfcc_vector13, mfcc_vector15]))\n",
    "print('[3.1 vs 3.1 noise 1]\\n유사도:', result([mfcc_vector13, mfcc_vector16]))\n",
    "print('[3.1 vs 3.1 noise 2]\\n유사도:', result([mfcc_vector13, mfcc_vector17]))\n",
    "print('[3.1 vs 다른 노래]\\n유사도:', result([mfcc_vector13, mfcc_vector37]))\n",
    "print('[3.1 vs 3.2]\\n유사도:', result([mfcc_vector13, mfcc_vector18]))\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "print('[4.1 vs 4.1 key 3]\\n유사도:', result([mfcc_vector19, mfcc_vector20]))\n",
    "print('[4.1 vs 4.1 key -3]\\n유사도:', result([mfcc_vector19, mfcc_vector21]))\n",
    "print('[4.1 vs 4.1 noise 1]\\n유사도:', result([mfcc_vector19, mfcc_vector22]))\n",
    "print('[4.1 vs 4.1 noise 2]\\n유사도:', result([mfcc_vector19, mfcc_vector23]))\n",
    "print('[4.1 vs 다른 노래]\\n유사도:', result([mfcc_vector19, mfcc_vector37]))\n",
    "print('[4.1 vs 4.2]\\n유사도:',result([mfcc_vector19, mfcc_vector24]))\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "print('[5.1 vs 5.1 key 3]\\n유사도:',result([mfcc_vector25, mfcc_vector26]))\n",
    "print('[5.1 vs 5.1 key -3]\\n유사도:', result([mfcc_vector25, mfcc_vector27]))\n",
    "print('[5.1 vs 5.1 noise 1]\\n유사도:', result([mfcc_vector25, mfcc_vector28]))\n",
    "print('[5.1 vs 5.1 noise 2]\\n유사도:', result([mfcc_vector25, mfcc_vector29]))\n",
    "print('[5.1 vs 다른 노래]\\n유사도:', result([mfcc_vector25, mfcc_vector37]))\n",
    "print('[5.1 vs 5.2]\\n유사도:',result([mfcc_vector25, mfcc_vector30]))\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "print('[6.1 vs 6.1 key 3]\\n유사도:', result([mfcc_vector31, mfcc_vector32]))\n",
    "print('[6.1 vs 6.1 key -3]\\n유사도:',result([mfcc_vector31, mfcc_vector33]))\n",
    "print('[6.1 vs 6.1 noise 1]\\n유사도:', result([mfcc_vector31, mfcc_vector34]))\n",
    "print('[6.1 vs 6.1 noise 2]\\n유사도:', result([mfcc_vector31, mfcc_vector35]))\n",
    "print('[6.1 vs 다른 노래]\\n유사도:', result([mfcc_vector31, mfcc_vector37]))\n",
    "print('[6.1 vs 6.2]\\n유사도:', result([mfcc_vector31, mfcc_vector36]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
