{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adc3405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Flatten, Dense, Dropout, Lambda, LSTM\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bb2200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(directory, max_files_per_class=16):\n",
    "\n",
    "    data_pairs = []\n",
    "    labels = []\n",
    "    all_class_vectors = []\n",
    "    \n",
    "    cnt_1=0\n",
    "    cnt_0=0\n",
    "\n",
    "    for class_folder in os.listdir(directory):\n",
    "        if class_folder == \".ipynb_checkpoints\":\n",
    "            continue\n",
    "        class_folder_path = os.path.join(directory, class_folder)\n",
    "\n",
    "        class_vectors = []\n",
    "        for filename in sorted(os.listdir(class_folder_path))[:max_files_per_class]:\n",
    "            if filename == \".ipynb_checkpoints\":\n",
    "                continue\n",
    "            file_path = os.path.join(class_folder_path, filename)\n",
    "            mfcc_vector = np.load(file_path)\n",
    "            class_vectors.append(mfcc_vector)\n",
    "\n",
    "        if len(class_vectors) > 0:\n",
    "            all_class_vectors.append(class_vectors)\n",
    "\n",
    "    for class_vectors in all_class_vectors:\n",
    "        for pair in combinations(class_vectors, 2):\n",
    "            data_pairs.append(pair)\n",
    "            labels.append(1)\n",
    "            cnt_1 += 1\n",
    "\n",
    "    for i, class_vectors in enumerate(all_class_vectors):\n",
    "        for j in range(i + 1, len(all_class_vectors)):\n",
    "            for pair in combinations(zip(class_vectors, all_class_vectors[j]), 2):\n",
    "                data_pairs.append((pair[0][0], pair[1][1]))\n",
    "                labels.append(0)\n",
    "                cnt_0 += 1\n",
    "    \n",
    "    print(cnt_1)\n",
    "    print(cnt_0)\n",
    "    \n",
    "    return data_pairs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aebf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_dataset_dir = \"E:\\\\UGRP\\\\npy\\\\mfcc\"\n",
    "\n",
    "data_pairs, labels = load_data(mfcc_dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f757eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pairs[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab44a2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_base_network(input_shape):\n",
    "    input = Input(shape=input_shape)\n",
    "    x = LSTM(128, return_sequences=True)(input)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    return Model(input, x)\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "def contrastive_loss_with_margin(margin):\n",
    "    def contrastive_loss(y_true, y_pred):\n",
    "        y_true = K.cast(y_true, 'float32')\n",
    "        square_pred = K.square(y_pred)\n",
    "        margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
    "        return K.mean(y_true * square_pred + (1 - y_true) * margin_square)\n",
    "    return contrastive_loss\n",
    "\n",
    "\n",
    "input_shape = (13, 2498)\n",
    "\n",
    "base_network = initialize_base_network(input_shape)\n",
    "\n",
    "input_a = Input(shape=input_shape)\n",
    "input_b = Input(shape=input_shape)\n",
    "\n",
    "\n",
    "processed_a = base_network(input_a)\n",
    "processed_b = base_network(input_b)\n",
    "\n",
    "distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
    "\n",
    "model = Model([input_a, input_b], distance)\n",
    "\n",
    "adam = Adam(learning_rate=0.001)\n",
    "model.compile(loss=contrastive_loss_with_margin(margin=1), optimizer=adam)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f99d551",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f141b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_pairs = []\n",
    "new_labels = []\n",
    "\n",
    "for i in range(len(data_pairs)):\n",
    "    if (data_pairs[i][0].shape == (13, 2498) and data_pairs[i][1].shape == (13, 2498)):\n",
    "        new_data_pairs.append(data_pairs[i])\n",
    "        new_labels.append(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b67b961",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pairs = new_data_pairs\n",
    "labels = new_labels\n",
    "len(data_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c4b947",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_num = 100000\n",
    "data_num = 10000  #한번에 학습시킬 데이터 개수\n",
    "\n",
    "k = len(data_pairs)/slice_num\n",
    "t = int(k)\n",
    "\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e029d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "random.seed(42)\n",
    "\n",
    "all_data_pairs = []\n",
    "all_labels = []\n",
    "for i in range(t):\n",
    "    if (i == t-1):\n",
    "        sample_data_pairs = data_pairs[i*slice_num:]\n",
    "        sample_labels = labels[i*slice_num:]\n",
    "        if (len(sample_data_pairs) > data_num):\n",
    "            sample_data_pairs = random.sample(sample_data_pairs, k=data_num)\n",
    "            sample_labels = random.sample(sample_labels, k=data_num)   \n",
    "    else:\n",
    "        sample_data_pairs = data_pairs[i*slice_num:(i+1)*slice_num]\n",
    "        sample_labels = labels[i*slice_num:(i+1)*slice_num]\n",
    "        sample_data_pairs = random.sample(sample_data_pairs, k=data_num)\n",
    "        sample_labels = random.sample(sample_labels, k=data_num)\n",
    "    all_data_pairs.append(sample_data_pairs)\n",
    "    all_labels.append(sample_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e373c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "for i in range(len(all_data_pairs)):\n",
    "    \n",
    "    print(i+1, \"training...\")\n",
    "    pairs = np.array(all_data_pairs[i])\n",
    "    labels = np.array(all_labels[i])\n",
    "    \n",
    "    pairs_train, pairs_test, labels_train, labels_test = train_test_split(pairs, labels, test_size=0.2, random_state=42)\n",
    "    pairs_train, pairs_val, labels_train, labels_val = train_test_split(pairs_train, labels_train, test_size=0.2, random_state=42)\n",
    "\n",
    "    early_stopping = EarlyStopping()\n",
    "\n",
    "    hist = model.fit([pairs_train[:, 0], pairs_train[:, 1]], labels_train,\n",
    "          validation_data=([pairs_val[:, 0], pairs_val[:, 1]], labels_val),\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          callbacks=[early_stopping])\n",
    "    train_loss.append(hist.history['loss'])\n",
    "    val_loss.append(hist.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127ce106",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_result = []\n",
    "for i in range(len(train_loss)):\n",
    "    for j in range(len(train_loss[i])):\n",
    "        train_loss_result.append(train_loss[i][j])\n",
    "\n",
    "val_loss_result = []\n",
    "for i in range(len(val_loss)):\n",
    "    for j in range(len(val_loss[i])):\n",
    "        val_loss_result.append(val_loss[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dde9c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_loss_result, label=\"Training loss\")\n",
    "plt.plot(val_loss_result, label=\"Val loss\")\n",
    "\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b300301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate([pairs_test[:, 0], pairs_test[:, 1]], labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00750818",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('C:\\\\Users\\\\new_folder\\\\models\\\\MFCC_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
